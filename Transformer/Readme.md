## âš¡ About the Files in the Transformer Folder

This folder contains the foundational code implementations of the **Transformer architecture**, built completely from scratch using TensorFlow/Keras.  
These files represent my learning and practice as part of the course **Generative AI for Application Building [CSU2220]**.

The focus here is on understanding **how a Transformer actually works internally**, without using high-level shortcuts or prebuilt models.

---

### ðŸ“„ Files Included

### **Transformer_Code (Only code No Inference or Training).ipynb**
This notebook contains a clean, minimal example of how the **encoder and decoder interact** inside a Transformer.

In this file, I learned:

- How the encoder processes input sequences  
- How the decoder uses masked self-attention  
- How cross-attention connects encoder â†’ decoder  
- How output logits are generated  

This is a structural and conceptual implementation meant for learning the internal flow of Transformer-based sequence-to-sequence models.

---

## ðŸš€ What I Learned from This Folder

- How attention works at a mathematical level  
- Why multi-head attention improves representation learning  
- How Transformers remove the need for RNNs/LSTMs  
- How encoderâ€“decoder models power applications like:
  - Translation  
  - Summarization  
  - Text generation  
- How modern LLMs are built using stacked Transformer layers  

---

## ðŸŽ¯ Purpose of This Folder

The Transformer notebooks in this folder are purely **educational and structural**.  
They helped me build an intuitive understanding of:

- Attention mechanisms  
- Self-attention vs cross-attention  
- Layer normalization  
- Masking techniques  
- Transformer forward pass  

These files form the foundation for building more advanced models like GPT-style decoders or T5-style encoderâ€“decoders.

---
