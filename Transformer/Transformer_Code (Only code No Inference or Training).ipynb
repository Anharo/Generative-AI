{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "ZvIZqNMSJVUT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zDGSEvcWJRku"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "CaKEUd7LJhsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(max_len, dm):\n",
        "    pos = np.arange(max_len)[:, np.newaxis]\n",
        "    i = np.arange(dm)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(dm))\n",
        "    angles = pos * angle_rates\n",
        "\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])  # even indices\n",
        "    angles[:, 1::2] = np.cos(angles[:, 1::2])  # odd indices\n",
        "\n",
        "    return tf.cast(angles[np.newaxis, ...], dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "IcSCouOmJgnb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "XrEfbnuaJoPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_attention(q, k, v, mask=None):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled = matmul_qk / tf.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled += (mask * -1e9)\n",
        "\n",
        "    weights = tf.nn.softmax(scaled, axis=-1)\n",
        "    output = tf.matmul(weights, v)\n",
        "    return output, weights\n"
      ],
      "metadata": {
        "id": "5tyUgrH-Jp9M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention Layer"
      ],
      "metadata": {
        "id": "2p3_cbwOJs81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, dm, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.dm = dm\n",
        "        self.depth = dm // num_heads\n",
        "\n",
        "        self.wq = layers.Dense(dm)\n",
        "        self.wk = layers.Dense(dm)\n",
        "        self.wv = layers.Dense(dm)\n",
        "        self.linear = layers.Dense(dm)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch = tf.shape(x)[0]\n",
        "        x = tf.reshape(x, (batch, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, [0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        q = self.split_heads(self.wq(q))\n",
        "        k = self.split_heads(self.wk(k))\n",
        "        v = self.split_heads(self.wv(v))\n",
        "\n",
        "        scaled, _ = scaled_dot_attention(q, k, v, mask)\n",
        "\n",
        "        scaled = tf.transpose(scaled, [0, 2, 1, 3])\n",
        "        concat = tf.reshape(scaled, (tf.shape(scaled)[0], -1, self.dm))\n",
        "\n",
        "        return self.linear(concat)\n"
      ],
      "metadata": {
        "id": "O415kNfvJu9V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed Forward Network"
      ],
      "metadata": {
        "id": "K7Hw8DLUJy1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward(dm, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        layers.Dense(dff, activation='relu'),\n",
        "        layers.Dense(dm)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "fL-M_O0EJxCZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder Block"
      ],
      "metadata": {
        "id": "8L61fz3kJ3qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(layers.Layer):\n",
        "    def __init__(self, dm, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(dm, num_heads)\n",
        "        self.ffn = feed_forward(dm, dff)\n",
        "\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.drop1 = layers.Dropout(rate)\n",
        "        self.drop2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        att = self.mha(x, x, x, mask)\n",
        "        att = self.drop1(att, training=training)\n",
        "        out1 = self.norm1(x + att)\n",
        "\n",
        "        ffn_out = self.ffn(out1)\n",
        "        ffn_out = self.drop2(ffn_out, training=training)\n",
        "\n",
        "        return self.norm2(out1 + ffn_out)\n"
      ],
      "metadata": {
        "id": "2b7XdXXCJ6CB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Block"
      ],
      "metadata": {
        "id": "9G58RD52Koon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(layers.Layer):\n",
        "    def __init__(self, dm, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(dm, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(dm, num_heads)\n",
        "\n",
        "        self.ffn = feed_forward(dm, dff)\n",
        "\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.drop1 = layers.Dropout(rate)\n",
        "        self.drop2 = layers.Dropout(rate)\n",
        "        self.drop3 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_out, training, look_ahead_mask, padding_mask):\n",
        "        # Masked self-attention (decoder)\n",
        "        att1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        att1 = self.drop1(att1, training=training)\n",
        "        out1 = self.norm1(att1 + x)\n",
        "\n",
        "        # Cross-attention (decoder queries, encoder keys/values)\n",
        "        att2 = self.mha2(enc_out, enc_out, out1, padding_mask)\n",
        "        att2 = self.drop2(att2, training=training)\n",
        "        out2 = self.norm2(att2 + out1)\n",
        "\n",
        "        # FFN\n",
        "        ffn_out = self.ffn(out2)\n",
        "        ffn_out = self.drop3(ffn_out, training=training)\n",
        "\n",
        "        return self.norm3(out2 + ffn_out)\n"
      ],
      "metadata": {
        "id": "wIA3NAa8Kpyi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "vF5bubhsKtSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, num_layers, dm, num_heads, dff, vocab, max_len, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = layers.Embedding(vocab, dm)\n",
        "        self.pos = positional_encoding(max_len, dm)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderBlock(dm, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.drop = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(tf.shape(x)[-1], tf.float32))\n",
        "        x += self.pos[:, :seq_len, :]\n",
        "\n",
        "        x = self.drop(x, training=training)\n",
        "\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training, mask)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "BGfxBChlKvtA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "M-IeUFxdKyMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, num_layers, dm, num_heads, dff, vocab, max_len, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab, dm)\n",
        "        self.pos = positional_encoding(max_len, dm)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderBlock(dm, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.dropout = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_out, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(tf.shape(x)[-1], tf.float32))\n",
        "        x += self.pos[:, :seq_len, :]\n",
        "\n",
        "        for layer in self.dec_layers:\n",
        "            x = layer(x, enc_out, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "BQVEO-o7KzVi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FULL Transformer Model (Encoder and Decoder)"
      ],
      "metadata": {
        "id": "_FP9l-y-J8DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, dm, num_heads, dff, input_vocab,\n",
        "                 target_vocab, max_len, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, dm, num_heads, dff, input_vocab, max_len)\n",
        "        self.decoder = Decoder(num_layers, dm, num_heads, dff, target_vocab, max_len)\n",
        "        self.final_layer = layers.Dense(target_vocab)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask,\n",
        "             look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_out = self.encoder(inp, training, enc_padding_mask)\n",
        "        dec_out = self.decoder(tar, enc_out, training,\n",
        "                               look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        return self.final_layer(dec_out)\n"
      ],
      "metadata": {
        "id": "DGsdb7mnJ_Jq"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}