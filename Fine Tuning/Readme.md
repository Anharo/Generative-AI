# üß† Fine-Tuning GPT-2 on *Feliz Pero ‚Äì Chapter 1*

This repository contains code for **fine-tuning GPT-2** on a custom dataset consisting of **Chapter 1 of my original story _"Feliz Pero"_**.  
The goal of this project is to observe how fine-tuning affects a language model when trained on a **small, narrative-style dataset written by the author**, and to demonstrate a complete fine-tuning pipeline using Hugging Face `transformers`.

> ‚úÖ This fine-tuning was done **for educational and experimental purposes only**  
> ‚úÖ The dataset is original content written by me

---

## üöÄ Features

- Fine-tuning GPT-2 using Hugging Face
- Custom narrative dataset (fictional story)
- Full training + text generation pipeline
- GPU/CPU compatible
- Easy template for fine-tuning on any text

---

## üìÇ Repository Structure

üìÅ GPT2-FineTune-FelizPero
‚îÇ
‚îú‚îÄ‚îÄ dataset/
‚îÇ ‚îî‚îÄ‚îÄ feliz.txt
‚îÇ
‚îú‚îÄ‚îÄ fine_tune.py
‚îî‚îÄ‚îÄ README.md

---

## üóÇÔ∏è Dataset

The dataset used for training contains:

- Chapter 1 of my story **Feliz Pero**
- Fictional, narrative writing
- Emotional and descriptive storytelling style

The dataset is formatted as a single `.txt` file for easy training.

> ‚ö†Ô∏è The story is **original content**, please do not redistribute without permission.

---
## Main libraries:

- transformers
- datasets
- torch
- accelerate
- tokenizer
